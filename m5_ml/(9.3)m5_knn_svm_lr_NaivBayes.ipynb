{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a3bf14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score , cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn. linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import make_classification\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_titanic_df, y_titanic_df, test_size=0.20, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82606cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "t_df = pd.read_pickle('./dataset/t_df.pkl')\n",
    "t_df\n",
    "\n",
    "X= t_df.drop('survived',axis=1)\n",
    "y= t_df['survived']\n",
    "\n",
    "# 독립변수 정규화(평균 0, 분산 1인 표준정규분포)\n",
    "X = preprocessing.StandardScaler().fit(X).transform(X)\n",
    "\n",
    "\n",
    "# 학습용 테이터와 평가용 데이터를 8:2로 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee284323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k 예측 정확도: 0.767175572519084\n"
     ]
    }
   ],
   "source": [
    "#knn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "k_model = KNeighborsClassifier(n_neighbors=5)\n",
    "k_model.fit(X_train,y_train)\n",
    "k_pred = k_model.predict(X_test)\n",
    "\n",
    "k_accuracy = accuracy_score(y_test, k_pred)\n",
    "print('k 예측 정확도:', k_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d61f32a",
   "metadata": {},
   "source": [
    "### 서포트 벡터 머신(Support Vector Machines, SVM)\n",
    "\n",
    "- 분류나 회귀, 이상치 탐지 등에 사용되는 강력한 머신러닝 알고리즘 중 하나입니다. SVM은 주로 분류 문제에 사용되며, 이 알고리즘의 핵심 아이디어는 데이터를 고차원 공간으로 변환하여 서로 다른 클래스 간의 최대 마진을 찾는 것입니다.\n",
    "\n",
    "- SVM은 데이터를 두 개의 클래스로 나누는 결정 경계(결정 초평면이라고도 함)를 찾습니다. 이 결정 경계는 각 클래스의 가장 가까운 훈련 샘플(서포트 벡터라고 함)까지의 거리가 최대가 되는 선을 찾는 것을 목표로 합니다. 이를 '마진 최대화'라고 하며, 이 마진 최대화는 오류를 최소화하고 모델의 일반화 성능을 향상시키는 데 중요한 역할을 합니다.\n",
    "\n",
    "- SVM은 선형 뿐만 아니라 비선형 분류 문제에도 사용할 수 있습니다. 비선형 문제를 해결하기 위해, SVM은 커널 트릭이라는 기법을 사용하여 데이터를 고차원 공간으로 변환하고, 그 고차원에서 선형 결정 경계를 찾습니다. 이 커널 트릭 덕분에 SVM은 복잡한 분류 문제를 처리할 수 있습니다.\n",
    "\n",
    "- SVM은 작은 데이터셋에서도 잘 작동하며, 높은 차원의 데이터에 대해 강력한 성능을 발휘합니다. 그러나 데이터셋이 크거나 노이즈가 많은 경우, 그리고 데이터가 선형적으로 구분되지 않는 경우에는 다른 알고리즘(예: 랜덤 포레스트나 신경망)에 비해 성능이 떨어질 수 있습니다.\n",
    "\n",
    "- SVR (adp 시험에 출제됨. 요건 몰랐지~?!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0947702",
   "metadata": {},
   "source": [
    "### 커널 함수의 역할\n",
    "\n",
    "- 차원의 확장: 데이터가 선형적으로 구분되지 않는 경우, 그 데이터를 더 높은 차원으로 매핑하여 선형적으로 분리할 수 있게 하는 것이 커널 함수의 주요 기능입니다. 예를 들어, 2차원 공간에서는 선형적으로 분리할 수 없는 데이터를 3차원 공간으로 매핑함으로써, 선형 분리가 가능해질 수 있습니다.\n",
    "\n",
    "- 컴퓨터 연산의 효율성: 커널 함수를 사용하면 실제로 데이터를 더 높은 차원으로 변환하는 복잡한 계산을 수행할 필요 없이, 원래의 차원에서 두 데이터 포인트의 유사도를 계산함으로써 동일한 효과를 얻을 수 있습니다. 이것을 '커널 트릭'이라고도 합니다. 이 방법은 컴퓨터 메모리와 연산 속도를 크게 절약할 수 있습니다.\n",
    "\n",
    "- 비선형 분리 가능: 실제 세계의 많은 데이터셋들은 선형적으로 완벽하게 분리할 수 없습니다. 커널 함수를 사용하면 이러한 비선형 데이터셋도 잘 분리할 수 있습니다.\n",
    "\n",
    "- 커널 함수는 여러 형태가 있는데, 선형 커널, 다항 커널, RBF(Radial Basis Function) 커널, 시그모이드 커널 등이 있습니다. 이들 중에서 가장 널리 사용되는 것은 RBF 커널입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b2e5a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "t_df = pd.read_pickle('./dataset/t_df.pkl')\n",
    "t_df\n",
    "\n",
    "X= t_df.drop('survived',axis=1)\n",
    "y= t_df['survived']\n",
    "\n",
    "X=preprocessing.StandardScaler().fit(X).transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "710d9bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S 정확도:  0.816793893129771\n"
     ]
    }
   ],
   "source": [
    "# svm (분류)\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score,classification_report\n",
    "from sklearn import svm\n",
    "\n",
    "svm_model = svm.SVC(kernel='rbf',random_state=0)\n",
    "svm_model.fit(X_train,y_train)\n",
    "s_pred = svm_model.predict(X_test)\n",
    "s_accuracy = accuracy_score(y_test,s_pred)\n",
    "print('S 정확도: ',s_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fd46ec",
   "metadata": {},
   "source": [
    "### 서포트 벡터 회귀(Support Vector Regression, SVR)\n",
    "- 서포트 벡터 머신(SVM)의 개념을 회귀 문제에 적용한 것입니다.\n",
    "\n",
    "- SVM이 두 클래스 사이의 마진을 최대화하는 방식으로 분류 문제를 해결하는 것과 달리, SVR은 이 마진 안에 가능한 한 많은 데이터 포인트가 포함되도록 회귀선을 조정합니다. 이 마진은 사용자가 정의한 오차 허용 범위(epsilon)에 의해 결정되며, 이 범위 안의 오차는 모델에게 패널티를 주지 않습니다.\n",
    "\n",
    "- SVR의 목표는 주어진 epsilon에 대해 최대한 많은 데이터 포인트가 떨어져 있는 회귀선을 찾는 것입니다. 이 회귀선은 최대한 많은 데이터 포인트를 epsilon 범위 안에 포함하려고 시도하며, 범위 밖의 데이터 포인트는 비용 함수를 통해 패널티를 부여받습니다.\n",
    "\n",
    "- SVR은 선형 회귀뿐만 아니라 비선형 회귀 문제에도 적용될 수 있습니다. 비선형 문제에서는 커널 트릭을 사용하여 데이터를 고차원으로 매핑하고, 이러한 고차원 공간에서 선형 회귀를 수행합니다.\n",
    "\n",
    "- SVR은 모델의 복잡성을 제어하는 데 중요한 두 가지 매개변수, 즉 C와 epsilon을 가지고 있습니다. C는 패널티 항의 강도를 제어하고(패널티를 주는 강도), epsilon은 마진의 폭을 제어합니다(패널티를 주는 범위). 이 매개변수들은 모델의 성능과 과적합 문제를 제어하는 데 중요한 역할을 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "342a36ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse:15.377 \n",
      "\n",
      "rmse:3.921 \n",
      "\n",
      "예측값:  18.522\n"
     ]
    }
   ],
   "source": [
    "# svr (회귀)\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "data = datasets.load_boston()\n",
    "X,y = shuffle(data.data, data.target, random_state=7)\n",
    "\n",
    "num_training = int(0.8*len(X))\n",
    "X_train,y_train=X[:num_training],y[:num_training]\n",
    "X_test, y_test = X[num_training:],y[num_training:]\n",
    "\n",
    "\n",
    "# create support vector regression model\n",
    "# kernel : 선형 커널\n",
    "# C: 학습 오류에 대한 패널티,C값이 클 수록 모델이 학습 데이터에 좀 더 최적화 됨, 너무 크면 오버피팅 발생\n",
    "# epsilon: 임계값, 예측한 값이 GT범위 안에 있으면 패널티 부여하지 않음.\n",
    "sv_regressor = SVR(kernel='linear',C=1.0, epsilon=0.1)\n",
    "\n",
    "sv_regressor.fit(X_train,y_train)\n",
    "\n",
    "y_pred = sv_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test,y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f'mse:{mse:.3f}','\\n')\n",
    "print(f'rmse:{rmse:.3f}','\\n')\n",
    "test_data = [3.7, 0, 18.4, 1, 0.87, 5.95, 91, 2.5052, 26, 666, 20.2, 351.34, 15.27]\n",
    "print('예측값: ',np.round(sv_regressor.predict([test_data])[0],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cc1a0c",
   "metadata": {},
   "source": [
    "### 로지스틱 회귀(Logistic Regression)\n",
    "\n",
    "- 분류 문제를 해결하기 위한 알고리즘으로, 선형 회귀와 마찬가지로 입력 특성의 가중치 합을 계산하는데, 선형 회귀와 달리 결과를 이진 분류(0 또는 1, 참 또는 거짓 등)에 사용할 수 있는 확률로 변환합니다.\n",
    "\n",
    "- 로지스틱 회귀는 선형 회귀와 비슷하게 모델의 예측과 실제 값 사이의 차이를 최소화하도록 가중치를 학습합니다. 하지만 로지스틱 회귀는 선형 회귀와는 달리 결과를 0과 1 사이의 값으로 제한하는 로지스틱 함수(또는 시그모이드 함수)를 사용합니다.\n",
    "\n",
    "- 로지스틱 함수를 그린게 s자 곡선\n",
    "\n",
    "- 로지스틱 회귀는 각 클래스에 속할 확률을 제공하며, 특정 임계값(일반적으로 0.5)을 초과하는 경우 데이터 포인트를 해당 클래스에 할당합니다. 이는 이진 분류뿐만 아니라 다중 클래스 분류에도 적용될 수 있습니다(이 경우에는 일대다(OvR) 또는 다항 로지스틱 회귀를 사용할 수 있습니다).\n",
    "\n",
    "- 로지스틱 회귀는 출력이 확률이기 때문에, 결과의 해석이 직관적이며 모델의 예측이 불확실한 경우에도 그 정도를 측정할 수 있습니다. 또한 로지스틱 회귀는 선형 회귀보다 이상치에 덜 민감하며, 모델이 과적합되는 것을 방지하기 위해 규제를 적용할 수 있다는 장점도 있습니다.\n",
    "\n",
    "- 로지스틱 함수, 또는 시그모이드 함수는 S-자 형태를 띠는 함수로, 실수 입력값을 0과 1 사이의 출력값으로 변환하는 데 사용됩니다. 이 함수는 머신러닝, 특히 이진 분류 문제에서 중요한 역할을 합니다. x가 0일 때 값은 0.5\n",
    "\n",
    "- 로지스틱 함수의 정의\n",
    "\n",
    "    - f(x) = 1 / (1 + e^-x)\n",
    "\n",
    "    - e는 자연 상수(약 2.71828)입니다. x는 어떤 실수 값도 가능하며, -무한대에서 무한대까지의 범위를 가집니다.\n",
    "\n",
    "    - 함수가 결과를 0과 1 사이로 제한하기 때문에, 이는 확률에 대해 논의할 때 특히 유용합니다. 로지스틱 회귀 분석에서 이 함수는 선형 함수의 결과를 확률로 변환하는데 사용됩니다.\n",
    "\n",
    "    - 입력값 x가 커질수록 로지스틱 함수의 출력은 1에 가까워지고, x가 작아질수록 출력은 0에 가까워집니다. x가 0일 때 로지스틱 함수의 값은 0.5입니다. 이러한 특성 때문에 로지스틱 함수는 이진 분류 문제에 널리 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "15ec0411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.982\n",
      "roc_auc: 0.980\n"
     ]
    }
   ],
   "source": [
    "# lr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(cancer.data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_scaled, cancer.target, test_size=0.20, random_state=11)\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train,y_train)\n",
    "lr_preds = lr_clf.predict(X_test)\n",
    "\n",
    "print('accuracy: {:0.3f}'.format(accuracy_score(y_test, lr_preds)))\n",
    "print('roc_auc: {:0.3f}'.format(roc_auc_score(y_test, lr_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5cbc37",
   "metadata": {},
   "source": [
    "### lr 주요 하이퍼파라미터\n",
    "\n",
    "- Regularization parameter (C): 로지스틱 회귀에서는 과적합을 방지하기 위해 정규화(regularization)를 사용합니다. 정규화는 모델의 복잡성에 대한 패널티를 부여하며, 이때 C는 이 패널티의 강도(세기)를 조정하는 역할을 합니다. C 값이 크면 패널티가 약해져(학습을 많이 함) 모델은 복잡해질 수 있으며, 이는 과적합의 가능성을 높일 수 있습니다. 반대로 C 값이 작으면 패널티가 강해져(학습 단순) 모델이 단순해질 수 있으며, 이는 과소적합의 가능성을 높일 수 있습니다.\n",
    "\n",
    "- Penalty: Penalty는 정규화의 형태를 결정하는 하이퍼파라미터입니다. 주로 L1, L2 두 가지 형태가 사용됩니다. L1 정규화는 변수의 수를 줄여 희소한 모델을 만드는데 효과적인 반면, L2 정규화는 모델의 가중치를 균등하게 줄여 과적합을 방지하는 데 효과적입니다. 이 두 정규화 방법은 로지스틱 회귀 모델의 성능과 특성을 다르게 만들 수 있으므로, 상황에 따라 적절하게 선택하는 것이 중요합니다.\n",
    "\n",
    "- 정규화\n",
    "    L1 정규화와 L2 정규화는 머신러닝 모델의 과적합을 방지하고 모델의 일반화 성능을 향상시키는 방법입니다. 이들은 모델의 가중치를 제한하여 모델의 복잡도를 제어하며, 가중치가 너무 커지는 것을 방지하여 노이즈에 대한 모델의 민감도를 감소시킵니다.\n",
    "   -  L1 정규화 (Lasso Regularization):\n",
    "        L1 정규화는 가중치 벡터의 L1 노름 (즉, 가중치의 절대값의 합)을 최소화하려고 합니다. 이는 비용 함수에 가중치의 절대값을 추가하는 형태로 이루어집니다. L1 정규화의 주요 특징 중 하나는 일부 가중치를 정확히 0으로 만들어 특성 선택의 효과를 갖는다는 것입니다. 이는 모델의 해석력을 향상시키고 불필요한 특성을 제거하여 모델을 단순화하는 데 도움이 됩니다.\n",
    "   -  L2 정규화 (Ridge Regularization):\n",
    "        L2 정규화는 가중치 벡터의 L2 노름 (즉, 가중치의 제곱합의 제곱근)을 최소화하려고 합니다. 이는 비용 함수에 가중치 제곱의 합을 추가하는 형태로 이루어집니다. L2 정규화는 가중치를 완전히 0으로 만들지 않지만, 모든 가중치를 작게 만듭니다. 이는 모델의 복잡도를 제한하면서도 모든 특성을 유지하게 하므로, L2 정규화는 L1 정규화보다 덜 희소한 모델을 생성합니다.\n",
    "\n",
    "    어떤 정규화를 선택할지는 문제와 데이터에 따라 달라집니다. L1 정규화는 특성 선택이 필요하거나 희소한 모델을 원할 때 유용하며, L2 정규화는 모든 특성이 관련성이 있을 것으로 예상되는 경우에 적합합니다. 실제로는 두 방법을 결합한 Elastic Net과 같은 방법을 사용하기도 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c4137dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적 하이퍼 파라미터:{'C': 1, 'penalty': 'l2'}, 최적 평균 정확도:0.975\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {'penalty':['l2','l1'], 'C':[0.01,0.1,1,5,10]}\n",
    "\n",
    "grid_clf = GridSearchCV(lr_clf, param_grid=params, scoring='accuracy', cv=3)\n",
    "grid_clf.fit(data_scaled, cancer.target)\n",
    "print('최적 하이퍼 파라미터:{0}, 최적 평균 정확도:{1:.3f}'.format(grid_clf.best_params_, grid_clf.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf512390",
   "metadata": {},
   "source": [
    "## 나이브 베이즈 분류 (Naive Bayes Classification)\n",
    "\n",
    "- 나이브 베이즈 분류기(Naive Bayes Classifier)는 베이즈의 정리를 적용한 간단한 확률적 분류 알고리즘입니다. 이 알고리즘은 모든 특성 값들이 서로 독립임을 가정하는데, 이 가정 때문에 '나이브(단순한)'라는 이름이 붙었습니다.\n",
    "\n",
    "- 나이브 베이즈 분류기 작동 방식:\n",
    "\n",
    "    - 각 클래스의 사전 확률을 계산합니다. 즉, 학습 데이터에서 각 클래스의 비율을 계산합니다.\n",
    "\n",
    "    - 각 특성에 대해, 각 클래스에서 그 특성의 확률을 계산합니다. 이를 \"우도(likelihood)\"라고 부릅니다.\n",
    "\n",
    "    - 새로운 데이터 포인트를 분류하려면, 모든 클래스에 대해 특성 값들의 우도를 곱하여 클래스의 사전 확률을 곱한 값(즉, 특성 값들이 주어졌을 때 클래스의 확률)을 계산합니다.\n",
    "\n",
    "    - 마지막으로, 가장 높은 확률을 가진 클래스를 예측 클래스로 선택합니다.\n",
    "\n",
    "- 작동 원리를 좀 더 직관적으로 설명하자면, 그것은 '투표'와 같은 원리로 이해할 수 있습니다. 각 특성이 클래스를 결정하는데 '표'를 주는 것처럼 생각해보세요.\n",
    "\n",
    "    - 사전 확률 계산: 먼저, 훈련 데이터에서 각 클래스의 비율을 계산합니다. 이를 각 클래스의 '기본 표'라고 생각할 수 있습니다. 예를 들어, 100개의 데이터 중에 30개가 클래스 A, 70개가 클래스 B에 속한다면, 클래스 A의 사전 확률은 0.3, 클래스 B의 사전 확률은 0.7입니다.\n",
    "\n",
    "    - 특성 별 확률 계산 (우도 계산): 각 클래스에서 각 특성의 확률을 계산합니다. 이는 각 특성이 클래스에 '표'를 주는 방식을 결정합니다. 예를 들어, '키'라는 특성이 있는데, 클래스 A에서는 '키가 큰' 사람의 비율이 높고, 클래스 B에서는 '키가 작은' 사람의 비율이 높다면, '키가 큰' 사람은 클래스 A에, '키가 작은' 사람은 클래스 B에 '표'를 줄 것입니다.\n",
    "\n",
    "    - 후보자 별 투표 집계: 새로운 데이터 포인트가 주어졌을 때, 모든 특성이 각 클래스에 '표'를 주는 방식을 따라 '표'를 계산합니다. 즉, 모든 특성의 '표'를 모두 더하고, 그것에 사전 확률(기본 표)을 더한 것이 각 클래스의 최종 '표'가 됩니다.\n",
    "\n",
    "    - 승리자 선정: 가장 많은 '표'를 받은 클래스가 새로운 데이터 포인트의 클래스가 됩니다.\n",
    "\n",
    "- 이처럼, 나이브 베이즈 분류기는 각 특성이 독립적으로 각 클래스에 '표'를 주는 것처럼 작동하므로, 이를 '나이브(단순한)' 베이즈 분류기라고 부릅니다.\n",
    "\n",
    "- 나이브 베이즈 분류기는 텍스트 분류(예: 스팸 메일 분류)에서 많이 사용되며, 계산이 간단하고 빠르며, 큰 데이터셋에 대해서도 잘 동작하는 특징을 가지고 있습니다. 하지만 나이브 베이즈의 '나이브'한 가정, 즉 모든 특성이 독립적이라는 가정은 실제 데이터에는 잘 맞지 않을 수 있으므로 이 점을 고려할 필요가 있습니다.\n",
    "- 나이브 베이즈는 스팸 메일 필터, 텍스트 분류, 감정 분석, 추천 시스템 등에 광범위하게 활용되는 분류 기법입니다. 나이브 베이즈 분류에 대해서 배우기 위해서는 베이즈 정리를 먼저 알아야 합니다.\n",
    "\n",
    "- 나이브 베이즈의 장점\n",
    "\n",
    "    - 간단하고, 빠르며, 정확한 모델입니다.\n",
    "    - computation cost가 작습니다. (따라서 빠릅니다.)\n",
    "    - 큰 데이터셋에 적합합니다.\n",
    "    - 연속형보다 이산형 데이터에서 성능이 좋습니다.\n",
    "    - Multiple class 예측을 위해서도 사용할 수 있습니다.\n",
    "\n",
    "- 단점\n",
    "    - feature 간의 독립성이 있어야 합니다. 하지만 실제 데이터에서 모든 feature가 독립인 경우는 드뭅니다. 장점이 많지만 feature가 서로 독립이어야 한다는 크리티컬한 단점이 있습니다.\n",
    "    - feature간 독립성이 있다는 말은 feature간에 서로 상관관계가 없다는 뜻입니다. \n",
    "    - X1과 X2라는 feature가 있을 때 X1이 증가하면 X2도 같이 증가한다고 합시다. 그럼 X1과 X2는 서로 상관관계가 있다고 말할 수 있고, 이는 X1과 X2가 독립성이 없다는 뜻입니다. \n",
    "    - X1과 X2가 독립성이 있으려면 X1이 증가하든 말든, X2에는 아무런 영향을 미치지 않아야 합니다. 하지만 우리가 얻을 수 있는 데이터에서는 feature간의 독립성이 항상 보장되지는 않습니다. \n",
    "    - 나이브 베이즈 모델은 feature간 독립성이 있다는 가정하에 성립되는 모델이기 때문에 실생활에서 바로 적용하기는 어려움있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbcc429",
   "metadata": {},
   "source": [
    "과제1_0522.\n",
    "\n",
    "어떤 마을 전체 사람들의 10.5%가 암 환자이고, 89.5%가 암 환자가 아닙니다. 이 마을의 모든 사람에 대해 암 검진을 실시했다고 합시다. 암 검진시 양성 판정, 음성 판정 결과가 나올 수 있습니다. 하지만 검진이 100% 정확하지는 않고 약간의 오차가 있습니다. 암 환자 중 양성 판정을 받은 비율은 90.5%, 암 환자 중 음성 판정을 받은 비율은 9.5%, 암 환자가 아닌 사람 중 양성 판정을 받은 비율은 20.4%, 암 환자가 아닌 사람 중 음성 판정을 받은 비율은 79.6%입니다. 어떤 사람이 양성 판정을 받았을 때 이 사람이 암 환자일 확률은 얼마일까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487c0c1c",
   "metadata": {},
   "source": [
    "주어진 정보를 바탕으로 양성 판정을 받은 사람이 암 환자일 확률을 계산해보겠습니다.\n",
    "\n",
    "주어진 확률과 조건은 다음과 같습니다:\n",
    "\n",
    "암 환자 비율: 10.5%\n",
    "암 환자 중 양성 판정 비율: 90.5%\n",
    "암 환자 중 음성 판정 비율: 9.5%\n",
    "암 환자가 아닌 사람 중 양성 판정 비율: 20.4%\n",
    "암 환자가 아닌 사람 중 음성 판정 비율: 79.6%\n",
    "양성 판정을 받은 사람 중 암 환자일 확률을 계산하기 위해서는 베이즈 정리를 사용할 수 있습니다.\n",
    "\n",
    "여기서 양성 판정을 받은 사람을 A, 암 환자를 B라고 하겠습니다.\n",
    "\n",
    "P(A|B)는 양성 판정을 받은 경우 암 환자일 확률을 나타냅니다.\n",
    "\n",
    "P(A)는 양성 판정을 받을 확률을, P(B)는 암 환자일 확률을 나타냅니다.\n",
    "\n",
    "베이즈 정리에 의하면,\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "여기서,\n",
    "\n",
    "P(B|A)는 암 환자 중 양성 판정을 받을 확률인 90.5% (0.905)\n",
    "P(A)는 암 환자 비율인 10.5% (0.105)\n",
    "P(B)는 양성 판정을 받을 확률로 계산할 수 있습니다.\n",
    "P(B)는 다음과 같이 계산할 수 있습니다:\n",
    "\n",
    "P(B) = P(B|A) * P(A) + P(B|~A) * P(~A)\n",
    "\n",
    "여기서,\n",
    "\n",
    "P(B|~A)는 암 환자가 아닌 사람 중 양성 판정을 받을 확률인 20.4% (0.204)\n",
    "P(~A)는 암 환자가 아닌 사람 비율로 계산할 수 있습니다 (1 - P(A)).\n",
    "따라서,\n",
    "\n",
    "P(B) = 0.905 * 0.105 + 0.204 * (1 - 0.105)\n",
    "\n",
    "이제 P(A|B)를 계산해보겠습니다:\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "P(A|B) = (0.905 * 0.105) / P(B)\n",
    "\n",
    "이를 계산하면:\n",
    "\n",
    "P(A|B) ≈ (0.095) / P(B)\n",
    "\n",
    "따라서, 양성 판정을 받은 사람이 암 환자일 확률은 0.095 / P(B)입니다. 이 값을 구하기 위해서는 P(B)를 계산해야 합니다. 위에서 계산한 P(B) 값을 대입하여 최종 확률을 계산할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93743df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전확률\n",
    "p_cancer = 0.105\n",
    "p_no_cancer=0.895\n",
    "\n",
    "p_positive_cancer=0.905\n",
    "p_negative_cancer=0.095\n",
    "p_positive_no_cancer=0.204\n",
    "p_negative_no_cancer=0.796\n",
    "\n",
    "p_positive = p_positive_cancer*p_cancer+p_positive_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90b1dd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나이브 베이즈 분류 문제 : 날씨가 overcast이고 기온이 mild일 때 경기할 확률은?\n",
    "\n",
    "weather=['Sunny','Sunny','Overcast','Rainy','Rainy','Rainy','Overcast','Sunny','Sunny',\n",
    "'Rainy','Sunny','Overcast','Overcast','Rainy']\n",
    "temp=['Hot','Hot','Hot','Mild','Cool','Cool','Cool','Mild','Cool','Mild','Mild','Mild','Hot','Mild']\n",
    "\n",
    "play=['No','No','Yes','Yes','Yes','No','Yes','No','Yes','Yes','Yes','Yes','Yes','No']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "072d8544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weat:  [2 2 0 1 1 1 0 2 2 1 2 0 0 1]\n",
      "temp:  [1 1 1 2 0 0 0 2 0 2 2 2 1 2]\n",
      "play:  [0 0 1 1 1 0 1 0 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# string을 int로 바꾸어 주는 features encoding을 해줍니다.\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "weather_encoded = le.fit_transform(weather)\n",
    "print('weat: ',weather_encoded)\n",
    "\n",
    "temp_encoded = le.fit_transform(temp)\n",
    "label = le.fit_transform(play)\n",
    "\n",
    "print('temp: ',temp_encoded)\n",
    "print('play: ',label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22f65ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 1), (2, 1), (0, 1), (1, 2), (1, 0), (1, 0), (0, 0), (2, 2), (2, 0), (1, 2), (2, 2), (0, 2), (0, 1), (1, 2)]\n"
     ]
    }
   ],
   "source": [
    "features = zip(weather_encoded, temp_encoded)\n",
    "features = list(features)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e5e236e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted value: [1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(features,label)\n",
    "predicted = model.predict([[0,2]])\n",
    "print('predicted value:',predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d503208",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
